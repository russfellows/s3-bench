# v0.5.1 Implementation Plan
**Target**: Machine-readable TSV metrics + Size-bucketed HDR histograms in workload runs

## Overview
Version 0.5.1 focuses on making io-bench results machine-parseable for automated analysis and consolidation across multiple test runs, plus ensuring consistent HDR histogram bucketing across all execution modes.

## Current State Assessment

### ‚úÖ Already Implemented
- **Phase 1: Prepare/Pre-population** (v0.4.3) - COMPLETE
  - `prepare_objects()`, `cleanup_prepared_objects()`
  - CLI flags: `--prepare-only`, `--no-cleanup`
  - Config schema: `PrepareConfig`, `EnsureSpec`
  
- **Phase 2: Advanced Replay Remapping** (v0.5.0) - COMPLETE
  - 4 rule types, 3 fanout strategies
  - Full remap engine implementation

- **Default concurrency = 20** (matches Warp)

### üîç Discovered Issues

#### HDR Histogram Inconsistency
**Problem**: Size-bucketed histograms only in CLI commands, NOT in workload runs

**Current State**:
- ‚úÖ CLI commands (`get`, `put`, `delete` in `src/main.rs`):
  - Uses `OpHists` with 9 size buckets
  - Records with `bucket_index(bytes.len())`
  - Prints per-size-bucket metrics
  
- ‚ùå Workload runs (`run` command in `src/workload.rs`):
  - Only 3 histograms: `hist_get`, `hist_put`, `hist_meta` (by operation type)
  - No size bucketing - just records raw microseconds
  - Missing granular size-based latency insights

**Impact**: Cannot compare latency characteristics across object sizes in workload runs

---

## v0.5.1 Feature Scope

### 1. Fix HDR Histogram Bucketing in Workload Runs (HIGH PRIORITY)

**Goal**: Make workload runs use same size-bucketed histograms as CLI commands

#### Implementation

**File**: `src/workload.rs`

**Changes**:
1. Replace 3 single histograms with 3 sets of bucketed histograms:
   ```rust
   // OLD (current)
   struct WorkerStats {
       hist_get: Histogram<u64>,
       hist_put: Histogram<u64>,
       hist_meta: Histogram<u64>,
       // ...
   }
   
   // NEW (v0.5.1)
   struct WorkerStats {
       hist_get: OpHists,   // 9 buckets
       hist_put: OpHists,   // 9 buckets
       hist_meta: OpHists,  // 9 buckets (for meta operations, usually bucket 0)
       // ...
   }
   ```

2. Update recording calls:
   ```rust
   // OLD
   ws.hist_get.record(micros.max(1)).ok();
   
   // NEW
   let bucket = bucket_index(bytes.len());
   ws.hist_get.record(bucket, duration);
   ```

3. Update stats printing:
   ```rust
   merged_get.print_summary("GET");
   merged_put.print_summary("PUT");
   merged_meta.print_summary("META");
   ```

**Benefit**: Detailed latency breakdowns by object size in workload runs

---

### 2. Machine-Readable TSV Results Output (HIGH PRIORITY)

**Goal**: Supplement human-readable output with TSV files for automated analysis

#### Design Philosophy
- **Keep existing output**: Don't remove pretty-printed human output
- **Add TSV export**: Via `--results-tsv <path>` flag
- **Multiple TSV files**: One per metric category
- **Easy parsing**: Tab-separated, clear headers, consistent format

#### TSV File Structure

**Proposed files** (all optional via single flag):

1. **`<basename>-summary.tsv`**: Overall test summary
   ```tsv
   metric	value	unit
   test_name	workload_mixed	string
   duration	300.123	seconds
   total_ops	125430	count
   total_bytes	131457280000	bytes
   ops_per_sec	418.1	ops/sec
   throughput_mbps	421.5	MB/s
   concurrency	32	workers
   timestamp	2025-10-04T12:34:56Z	iso8601
   ```

2. **`<basename>-operations.tsv`**: Per-operation-type metrics
   ```tsv
   operation	count	bytes	ops_per_sec	throughput_mbps	p50_us	p95_us	p99_us	max_us
   GET	62715	104857600000	209.05	336.2	1250	5890	12340	87650
   PUT	37629	26214400000	125.43	84.3	2340	8760	18920	145230
   LIST	12543	0	41.81	0.0	890	3450	7890	34560
   STAT	6272	0	20.91	0.0	670	2340	5670	23450
   DELETE	6271	0	20.91	0.0	450	1890	4560	18900
   ```

3. **`<basename>-size-buckets.tsv`**: Per-operation, per-size-bucket latencies
   ```tsv
   operation	size_bucket	count	p50_us	p95_us	p99_us	max_us
   GET	zero	0	-	-	-	-
   GET	1B-8KiB	1234	890	2340	4560	12340
   GET	8KiB-64KiB	5678	1230	3450	6780	18900
   GET	64KiB-512KiB	12345	2340	5670	11230	34560
   GET	512KiB-4MiB	23456	4560	12340	23450	67890
   GET	4MiB-32MiB	15678	8900	23450	45670	134560
   GET	32MiB-256MiB	4321	17800	56780	123450	345670
   GET	256MiB-2GiB	123	45670	134560	234560	567890
   GET	>2GiB	0	-	-	-	-
   PUT	zero	0	-	-	-	-
   PUT	1B-8KiB	234	1230	3450	6780	18900
   ...
   ```

4. **`<basename>-config.yaml`**: Copy of exact config used (for reproducibility)
   ```yaml
   # Auto-generated from test run 2025-10-04T12:34:56Z
   duration: 300s
   concurrency: 32
   workload:
     - weight: 50
       op: get
       path: "test/*"
   ...
   ```

#### CLI Integration

**File**: `src/main.rs`

Add flag to `Run` command:
```rust
#[derive(Args)]
struct RunArgs {
    // ... existing args ...
    
    /// Export results to TSV files (basename, creates multiple .tsv files)
    #[arg(long = "results-tsv")]
    results_tsv: Option<PathBuf>,
}
```

**Usage Examples**:
```bash
# Basic workload run (existing behavior)
io-bench run --config mixed.yaml

# With TSV export
io-bench run --config mixed.yaml --results-tsv results/test1

# Creates:
#   results/test1-summary.tsv
#   results/test1-operations.tsv
#   results/test1-size-buckets.tsv
#   results/test1-config.yaml
```

#### Implementation Details

**New module**: `src/tsv_export.rs`

```rust
use std::fs::File;
use std::io::Write;
use std::path::Path;
use anyhow::{Context, Result};

pub struct TsvExporter {
    basename: String,
}

impl TsvExporter {
    pub fn new(basename: impl AsRef<str>) -> Self {
        Self {
            basename: basename.as_ref().to_string(),
        }
    }
    
    pub fn export_summary(&self, stats: &WorkloadStats) -> Result<()> {
        let path = format!("{}-summary.tsv", self.basename);
        let mut f = File::create(&path)
            .with_context(|| format!("Failed to create {}", path))?;
        
        writeln!(f, "metric\tvalue\tunit")?;
        writeln!(f, "test_name\t{}\tstring", self.basename)?;
        writeln!(f, "duration\t{:.3}\tseconds", stats.duration.as_secs_f64())?;
        // ... more metrics ...
        
        Ok(())
    }
    
    pub fn export_operations(&self, get: &OpStats, put: &OpStats, meta: &OpStats) -> Result<()> {
        let path = format!("{}-operations.tsv", self.basename);
        let mut f = File::create(&path)?;
        
        writeln!(f, "operation\tcount\tbytes\tops_per_sec\tthroughput_mbps\tp50_us\tp95_us\tp99_us\tmax_us")?;
        self.write_op_row(&mut f, "GET", get)?;
        self.write_op_row(&mut f, "PUT", put)?;
        self.write_op_row(&mut f, "META", meta)?;
        
        Ok(())
    }
    
    pub fn export_size_buckets(&self, get: &OpHists, put: &OpHists, meta: &OpHists) -> Result<()> {
        let path = format!("{}-size-buckets.tsv", self.basename);
        let mut f = File::create(&path)?;
        
        writeln!(f, "operation\tsize_bucket\tcount\tp50_us\tp95_us\tp99_us\tmax_us")?;
        self.write_bucket_rows(&mut f, "GET", get)?;
        self.write_bucket_rows(&mut f, "PUT", put)?;
        self.write_bucket_rows(&mut f, "META", meta)?;
        
        Ok(())
    }
    
    pub fn export_config(&self, config_yaml: &str) -> Result<()> {
        let path = format!("{}-config.yaml", self.basename);
        let mut f = File::create(&path)?;
        writeln!(f, "# Auto-generated from test run {}", chrono::Utc::now())?;
        write!(f, "{}", config_yaml)?;
        Ok(())
    }
    
    fn write_op_row(&self, f: &mut File, op: &str, stats: &OpStats) -> Result<()> {
        writeln!(f, "{}\t{}\t{}\t{:.2}\t{:.2}\t{}\t{}\t{}\t{}",
            op,
            stats.count,
            stats.bytes,
            stats.ops_per_sec,
            stats.throughput_mbps,
            stats.p50_us,
            stats.p95_us,
            stats.p99_us,
            stats.max_us
        )?;
        Ok(())
    }
    
    fn write_bucket_rows(&self, f: &mut File, op: &str, hists: &OpHists) -> Result<()> {
        for (i, bucket_name) in BUCKET_LABELS.iter().enumerate() {
            let hist = hists.buckets[i].lock().unwrap();
            let count = hist.len();
            
            if count == 0 {
                writeln!(f, "{}\t{}\t0\t-\t-\t-\t-", op, bucket_name)?;
            } else {
                writeln!(f, "{}\t{}\t{}\t{}\t{}\t{}\t{}",
                    op,
                    bucket_name,
                    count,
                    hist.value_at_quantile(0.50),
                    hist.value_at_quantile(0.95),
                    hist.value_at_quantile(0.99),
                    hist.max()
                )?;
            }
        }
        Ok(())
    }
}
```

**Integration**: In `src/workload.rs`, after printing results:
```rust
// At end of run_workload(), after printing human-readable output
if let Some(tsv_path) = results_tsv {
    info!("Exporting results to TSV files: {}", tsv_path.display());
    let exporter = TsvExporter::new(tsv_path.to_string_lossy());
    exporter.export_summary(&overall_stats)?;
    exporter.export_operations(&get_stats, &put_stats, &meta_stats)?;
    exporter.export_size_buckets(&merged_get, &merged_put, &merged_meta)?;
    exporter.export_config(&original_config_yaml)?;
    println!("\n‚úÖ TSV results exported to: {}-*.tsv", tsv_path.display());
}
```

---

### 3. Update Default Concurrency: 20 ‚Üí 32 (TRIVIAL)

**File**: `src/config.rs`

**Change**:
```rust
fn default_concurrency() -> usize {
    32  // User preference (was 20 for Warp parity)
}
```

**Rationale**: User preference for higher default parallelism

---

### 4. UX Polish (MEDIUM PRIORITY)

#### 4.1 Update README with Examples

**File**: `README.md`

Add section:
```markdown
## Machine-Readable Results

Export detailed metrics in TSV format for automated analysis:

```bash
# Run test and export results
io-bench run --config bench.yaml --results-tsv results/run1

# Creates multiple TSV files:
#   results/run1-summary.tsv       # Overall metrics
#   results/run1-operations.tsv    # Per-operation stats
#   results/run1-size-buckets.tsv  # Latency by object size
#   results/run1-config.yaml       # Exact config used
```

### Analyzing Results with Standard Tools

```bash
# View summary
column -t -s $'\t' results/run1-summary.tsv

# Compare multiple runs
paste results/run1-operations.tsv results/run2-operations.tsv | column -t

# Plot latencies by size
python3 plot_latencies.py results/run1-size-buckets.tsv
```
```

#### 4.2 Enhanced CLI Help Text

**File**: `src/main.rs`

Update command documentation:
```rust
/// Run a mixed workload from YAML configuration
///
/// Examples:
///   # Basic workload
///   io-bench run --config mixed.yaml
///
///   # With TSV export for automated analysis
///   io-bench run --config mixed.yaml --results-tsv results/test1
///
///   # Pre-populate objects only
///   io-bench run --config mixed.yaml --prepare-only
///
///   # Run without cleanup (keep prepared objects)
///   io-bench run --config mixed.yaml --no-cleanup
#[derive(Args)]
struct RunArgs {
    // ...
}
```

---

## Implementation Timeline

### Sprint 1: HDR Histogram Fix (2-3 days)
- [ ] Move `OpHists` and `bucket_index()` to `src/lib.rs` or new `src/metrics.rs`
- [ ] Update `WorkerStats` in `src/workload.rs` to use `OpHists`
- [ ] Fix all recording calls to include bucket index
- [ ] Update stats merging and printing
- [ ] Test with example workload configs
- [ ] Verify output matches CLI command format

### Sprint 2: TSV Export (3-4 days)
- [ ] Create `src/tsv_export.rs` module
- [ ] Implement `TsvExporter` with 4 export methods
- [ ] Add `--results-tsv` CLI flag
- [ ] Integrate into `run_workload()` function
- [ ] Add `chrono` dependency for timestamps
- [ ] Test TSV file generation
- [ ] Validate TSV parsing with standard tools

### Sprint 3: UX Polish (1-2 days)
- [ ] Update default concurrency to 32
- [ ] Update README with TSV examples
- [ ] Enhance CLI help text
- [ ] Add example analysis scripts (optional)
- [ ] Update CHANGELOG

### Sprint 4: Testing & Release (1-2 days)
- [ ] Integration test: workload run with TSV export
- [ ] Verify histogram bucketing consistency
- [ ] Test all TSV file formats
- [ ] Performance validation (ensure no regression)
- [ ] Update version to 0.5.1
- [ ] Create release notes

**Total Estimate**: 7-11 days

---

## Acceptance Criteria

### Functional Requirements
- [ ] Workload runs use size-bucketed histograms (9 buckets per operation type)
- [ ] TSV export creates 4 files (summary, operations, size-buckets, config)
- [ ] TSV files are valid tab-separated format
- [ ] TSV files can be parsed by standard tools (`column`, `awk`, `python pandas`)
- [ ] Human-readable output remains unchanged (backward compatible)
- [ ] Default concurrency is 32

### Quality Requirements
- [ ] Zero performance regression in workload execution
- [ ] TSV export adds <100ms overhead
- [ ] All existing tests continue to pass
- [ ] New integration test for TSV export

### Documentation Requirements
- [ ] README includes TSV export examples
- [ ] CLI help text updated
- [ ] CHANGELOG entry for v0.5.1
- [ ] TSV file format documented

---

## Dependencies

**New Cargo dependencies**:
```toml
[dependencies]
chrono = "0.4"  # For ISO8601 timestamps in TSV exports
```

---

## Risk Mitigation

### Risk: Breaking existing histogram code
**Mitigation**: 
- Extract `OpHists` to shared module first
- Test CLI commands still work before changing workload
- Incremental changes with testing at each step

### Risk: TSV format compatibility
**Mitigation**:
- Use standard TSV format (tabs only, no spaces)
- Quote fields with special characters
- Test with multiple parsing tools (awk, pandas, Excel)

### Risk: File system errors during TSV export
**Mitigation**:
- Validate path before starting workload
- Use atomic writes (write to temp, then rename)
- Clear error messages if export fails

---

## Future Enhancements (Post-v0.5.1)

1. **JSON export option**: `--results-json` for programmatic parsing
2. **SQLite export**: `--results-db` for queryable results database
3. **CSV export**: `--results-csv` for Excel compatibility
4. **Plotting integration**: Auto-generate latency graphs
5. **Comparison tool**: `io-bench compare run1.tsv run2.tsv`
6. **Aggregation tool**: `io-bench aggregate run*.tsv > combined.tsv`

---

## Conclusion

v0.5.1 makes io-bench production-ready for automated benchmarking workflows by:
1. Ensuring consistent, detailed HDR histogram collection across all modes
2. Enabling machine-readable result export for analysis automation
3. Maintaining backward compatibility with existing workflows

This positions io-bench as a complete benchmarking solution for both interactive and automated use cases.
