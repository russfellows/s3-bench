# v0.6.0: Distributed Multi-Host Workload Design

**Date**: October 7, 2025  
**Status**: Design Phase  
**Target Release**: v0.6.0

## Overview

Design and implementation plan for distributed multi-host workload execution, enabling coordinated benchmarking across multiple agents using a shared YAML configuration file.

## Current State Analysis

### Existing Infrastructure ✅

1. **gRPC Communication Layer**
   - Protocol: `proto/iobench.proto`
   - Agent binary: `sai3bench-agent` (gRPC server)
   - Controller binary: `sai3bench-ctl` (gRPC client)
   - Messages: `Ping`, `RunGet`, `RunPut`
   - TLS support with self-signed certificates

2. **YAML Configuration System**
   - Main binary: `sai3-bench` (single-node CLI)
   - Config structure in `src/config.rs`
   - Workload execution in `src/workload.rs`
   - Supports: GET, PUT, LIST, STAT, DELETE operations
   - Features: Size distributions, prepare phase, weighted operations

3. **Current Distributed Capabilities**
   - Controller can send simple GET/PUT commands to multiple agents
   - Agents execute operations independently
   - Basic operation: `sai3bench-ctl --agents host1:7761,host2:7761 get --uri s3://bucket/path/*`

### Current Limitations ❌

1. **No YAML Config Distribution**
   - Controller only supports simple CLI commands (get, put, ping)
   - Cannot send full workload configurations to agents
   - No `RunWorkload` RPC message defined

2. **No Path Isolation**
   - All agents operate on same paths
   - Risk of conflicts (e.g., DELETE operations affecting GET operations)
   - No per-agent subdirectory support

3. **No Coordination**
   - Agents execute independently without synchronization
   - No shared start time for coordinated load generation
   - No aggregated results reporting

4. **Limited Operation Support**
   - Only GET and PUT exposed via gRPC
   - Missing: LIST, STAT, DELETE, weighted workloads

## Design Goals for v0.6.0

### Primary Objectives

1. **Distributed YAML Workload Execution**
   - Controller reads YAML config file
   - Distributes workload configuration to all agents
   - Agents execute same workload definition
   - Coordinated start time across all agents

2. **Per-Agent Path Isolation** (Key Innovation!)
   - Each agent operates in its own subdirectory/prefix
   - Prevents cross-agent interference
   - Enables truly independent parallel loads
   - Example:
     - Agent 1: `s3://bucket/bench/agent-1/`
     - Agent 2: `s3://bucket/bench/agent-2/`
     - Agent 3: `s3://bucket/bench/agent-3/`

3. **Aggregated Results**
   - Collect metrics from all agents
   - Combine into single summary output
   - Per-agent and total throughput/latency reporting
   - Export combined TSV results

4. **Backward Compatibility**
   - Single-node `sai3-bench` CLI unchanged
   - Existing configs work without modification
   - Optional distributed mode via controller

## Proposed Architecture

### Option A: YAML Config Distribution (Recommended)

**Pros:**
- Leverages existing workload execution code
- Full feature parity with single-node mode
- Simple agent implementation (reuse existing `run()` function)
- Easy to extend with new operation types

**Cons:**
- Requires serializing full config to protobuf or JSON
- Slightly larger messages over gRPC

**Implementation:**
```proto
message RunWorkloadRequest {
  string config_yaml = 1;      // Entire YAML config as string
  string agent_id = 2;          // Unique identifier for this agent (e.g., "agent-1")
  string path_prefix = 3;       // Per-agent path isolation (e.g., "agent-1/")
  int64 start_timestamp_ns = 4; // Coordinated start time (nanoseconds since epoch)
}

message WorkloadSummary {
  string agent_id = 1;
  double wall_seconds = 2;
  uint64 total_ops = 3;
  uint64 total_bytes = 4;
  OpAggregateMetrics get = 5;
  OpAggregateMetrics put = 6;
  OpAggregateMetrics meta = 7;
  // Histograms can be serialized as JSON or binary
}

message OpAggregateMetrics {
  uint64 bytes = 1;
  uint64 ops = 2;
  uint64 mean_us = 3;
  uint64 p50_us = 4;
  uint64 p95_us = 5;
  uint64 p99_us = 6;
}
```

### Option B: Operation-by-Operation Distribution

**Pros:**
- Finer-grained control
- Smaller messages

**Cons:**
- Complex protocol for all operation types
- Duplicate code between single/distributed modes
- Harder to maintain feature parity

**Decision: Choose Option A** - YAML distribution is simpler and more maintainable.

## Implementation Plan

### Phase 1: Protocol Extension (Week 1)

1. **Update `proto/iobench.proto`**
   - Add `RunWorkloadRequest` message
   - Add `WorkloadSummary` response message
   - Add `RunWorkload` RPC method to Agent service

2. **Rebuild gRPC stubs**
   - Run `build.rs` to regenerate `src/pb/iobench.rs`

### Phase 2: Agent Enhancement (Week 1)

1. **Implement `run_workload` RPC handler in `src/bin/agent.rs`**
   ```rust
   async fn run_workload(&self, req: Request<RunWorkloadRequest>) 
       -> Result<Response<WorkloadSummary>, Status> {
       let RunWorkloadRequest { config_yaml, agent_id, path_prefix, start_timestamp_ns } = req.into_inner();
       
       // 1. Parse YAML config
       let mut config: Config = serde_yaml::from_str(&config_yaml)?;
       
       // 2. Apply path prefix to all operations (PATH ISOLATION!)
       config.apply_agent_prefix(&agent_id, &path_prefix)?;
       
       // 3. Wait until coordinated start time
       wait_until_timestamp(start_timestamp_ns)?;
       
       // 4. Execute workload (reuse existing code!)
       let summary = workload::run(&config).await?;
       
       // 5. Convert Summary to WorkloadSummary protobuf
       Ok(Response::new(summary.to_proto()))
   }
   ```

2. **Add path rewriting logic to `src/config.rs`**
   ```rust
   impl Config {
       /// Apply agent-specific path prefix to all operations
       pub fn apply_agent_prefix(&mut self, agent_id: &str, prefix: &str) -> Result<()> {
           // Update target if set
           if let Some(ref target) = self.target {
               self.target = Some(join_paths(target, prefix)?);
           }
           
           // Update all operation paths
           for op in &mut self.workload {
               match &mut op.spec {
                   OpSpec::Get { path } => *path = join_paths(path, prefix)?,
                   OpSpec::Put { path, .. } => *path = join_paths(path, prefix)?,
                   OpSpec::List { path } => *path = join_paths(path, prefix)?,
                   OpSpec::Stat { path } => *path = join_paths(path, prefix)?,
                   OpSpec::Delete { path } => *path = join_paths(path, prefix)?,
               }
           }
           
           // Update prepare config if present
           if let Some(ref mut prepare) = self.prepare {
               prepare.target = join_paths(&prepare.target, prefix)?;
           }
           
           Ok(())
       }
   }
   ```

### Phase 3: Controller Enhancement (Week 2)

1. **Add `run` subcommand to `src/bin/controller.rs`**
   ```rust
   Commands::Run {
       config: String,        // Path to YAML config file
       path_template: Option<String>,  // Template for per-agent paths (e.g., "agent-{id}/")
   } => {
       // 1. Read YAML config
       let config_yaml = fs::read_to_string(&config)?;
       
       // 2. Calculate coordinated start time (2 seconds in future)
       let start_time = SystemTime::now() + Duration::from_secs(2);
       let start_ns = start_time.duration_since(UNIX_EPOCH)?.as_nanos() as i64;
       
       // 3. Send workload to all agents in parallel
       let mut handles = vec![];
       for (idx, agent_addr) in agents.iter().enumerate() {
           let agent_id = format!("agent-{}", idx + 1);
           let path_prefix = path_template
               .as_ref()
               .map(|t| t.replace("{id}", &agent_id))
               .unwrap_or(agent_id.clone() + "/");
           
           let client = mk_client(...).await?;
           let yaml = config_yaml.clone();
           
           handles.push(tokio::spawn(async move {
               let response = client.run_workload(RunWorkloadRequest {
                   config_yaml: yaml,
                   agent_id,
                   path_prefix,
                   start_timestamp_ns: start_ns,
               }).await?;
               Ok::<WorkloadSummary, anyhow::Error>(response.into_inner())
           }));
       }
       
       // 4. Collect results from all agents
       let mut summaries = vec![];
       for handle in handles {
           summaries.push(handle.await??);
       }
       
       // 5. Aggregate and display results
       print_distributed_results(&summaries);
   }
   ```

2. **Implement result aggregation**
   ```rust
   fn print_distributed_results(summaries: &[WorkloadSummary]) {
       println!("\n=== Distributed Results ===");
       println!("Total agents: {}", summaries.len());
       
       // Per-agent results
       for summary in summaries {
           println!("\nAgent: {}", summary.agent_id);
           println!("  Ops: {} ({:.2} ops/s)", summary.total_ops, 
                    summary.total_ops as f64 / summary.wall_seconds);
           println!("  Bytes: {:.2} MB", summary.total_bytes as f64 / 1_048_576.0);
           // ... detailed metrics
       }
       
       // Aggregated totals
       let total_ops: u64 = summaries.iter().map(|s| s.total_ops).sum();
       let total_bytes: u64 = summaries.iter().map(|s| s.total_bytes).sum();
       let max_wall = summaries.iter().map(|s| s.wall_seconds).fold(0.0, f64::max);
       
       println!("\n=== Aggregate Total ===");
       println!("Total ops: {} ({:.2} ops/s)", total_ops, total_ops as f64 / max_wall);
       println!("Total bytes: {:.2} MB ({:.2} MiB/s)", 
                total_bytes as f64 / 1_048_576.0,
                (total_bytes as f64 / 1_048_576.0) / max_wall);
   }
   ```

### Phase 4: Path Isolation Strategy

**Key Concept**: Each agent operates in an isolated namespace to prevent interference.

**Implementation Strategies**:

1. **Subdirectory Isolation** (Recommended for S3/Cloud)
   ```yaml
   # Original config
   target: "s3://benchmark-bucket/shared-prefix/"
   
   # Agent 1 sees:
   target: "s3://benchmark-bucket/shared-prefix/agent-1/"
   
   # Agent 2 sees:
   target: "s3://benchmark-bucket/shared-prefix/agent-2/"
   ```

2. **Bucket Isolation** (Alternative for multi-tenancy)
   ```yaml
   # Template in controller: --bucket-template "bench-{id}"
   # Agent 1 sees: s3://bench-agent-1/
   # Agent 2 sees: s3://bench-agent-2/
   ```

3. **Prefix Injection** (For file:// and direct://)
   ```yaml
   # Original: file:///mnt/bench/data/
   # Agent 1: file:///mnt/bench/agent-1/data/
   # Agent 2: file:///mnt/bench/agent-2/data/
   ```

**Configuration Options**:

```bash
# Option 1: Automatic agent-N subdirectories
sai3bench-ctl --agents host1:7761,host2:7761 run --config workload.yaml

# Option 2: Custom path template
sai3bench-ctl --agents host1:7761,host2:7761 run --config workload.yaml \
  --path-template "worker-{id}/"

# Option 3: Explicit agent IDs
sai3bench-ctl --agents host1:7761,host2:7761 run --config workload.yaml \
  --agent-ids "db-node-1,db-node-2"
```

### Phase 5: Testing & Validation (Week 3)

1. **Unit Tests**
   - Test path rewriting logic in `Config::apply_agent_prefix()`
   - Test URI joining for all backend types (s3://, file://, direct://)

2. **Integration Tests**
   - Multi-agent gRPC test with workload execution
   - Verify path isolation (agents don't see each other's objects)
   - Test coordinated start time (all agents start within 100ms)

3. **End-to-End Test**
   ```bash
   # Terminal 1: Start agent 1
   sai3bench-agent --listen 127.0.0.1:7761
   
   # Terminal 2: Start agent 2
   sai3bench-agent --listen 127.0.0.1:7762
   
   # Terminal 3: Run distributed workload
   sai3bench-ctl --insecure --agents 127.0.0.1:7761,127.0.0.1:7762 \
     run --config tests/configs/file_test.yaml
   
   # Verify:
   # - Both agents execute workload
   # - Results are aggregated
   # - No path conflicts
   ```

## Example Usage Scenarios

### Scenario 1: S3 Performance Testing with 3 Agents

```bash
# Start agents on 3 different hosts
# host1: sai3bench-agent --listen 0.0.0.0:7761
# host2: sai3bench-agent --listen 0.0.0.0:7761
# host3: sai3bench-agent --listen 0.0.0.0:7761

# Run coordinated workload from controller
sai3bench-ctl --agents host1:7761,host2:7761,host3:7761 \
  run --config s3-mixed-workload.yaml

# Output:
# === Distributed Results ===
# Total agents: 3
#
# Agent: agent-1
#   Ops: 50000 (16666.67 ops/s)
#   Bytes: 512.00 MB (170.67 MiB/s)
#   Path: s3://bucket/bench/agent-1/
#
# Agent: agent-2
#   Ops: 50000 (16666.67 ops/s)
#   Bytes: 512.00 MB (170.67 MiB/s)
#   Path: s3://bucket/bench/agent-2/
#
# Agent: agent-3
#   Ops: 50000 (16666.67 ops/s)
#   Bytes: 512.00 MB (170.67 MiB/s)
#   Path: s3://bucket/bench/agent-3/
#
# === Aggregate Total ===
# Total ops: 150000 (50000.00 ops/s)
# Total bytes: 1536.00 MB (512.00 MiB/s)
# Combined throughput: 3x single-agent baseline
```

### Scenario 2: File Backend with Direct I/O

```yaml
# config: distributed-direct.yaml
target: "direct:///mnt/nvme/bench/"
duration: "30s"
concurrency: 8

workload:
  - op: put
    path: "data/"
    weight: 50
    size_distribution:
      type: lognormal
      mean: 262144
      std_dev: 131072
  
  - op: get
    path: "data/*"
    weight: 50
```

```bash
# Each agent writes to isolated directory:
# Agent 1: /mnt/nvme/bench/agent-1/data/
# Agent 2: /mnt/nvme/bench/agent-2/data/
# No conflicts, true parallel I/O testing
```

## Configuration File Extensions (Optional)

For advanced use cases, we could extend the YAML format:

```yaml
# Future: Explicit distributed configuration
target: "s3://bucket/bench/"
duration: "60s"
concurrency: 4

# Optional distributed section (v0.7.0+)
distributed:
  mode: "isolated"  # Each agent gets own subdirectory
  # OR mode: "shared"  # All agents share same paths (for read-only tests)
  
  agent_naming: "agent-{id}"  # Default template
  # OR agent_naming: "custom-{hostname}-{id}"

workload:
  - op: get
    path: "data/*"
    weight: 100
```

**Decision**: Start without YAML extensions. Use controller CLI flags for distributed settings to keep configs portable between single/distributed modes.

## Migration Path

### v0.6.0 Release Scope

**Must Have:**
- ✅ RunWorkload RPC in protocol
- ✅ Agent workload execution with path isolation
- ✅ Controller `run` subcommand
- ✅ Basic result aggregation
- ✅ Integration tests

**Nice to Have:**
- TSV export aggregation across agents
- Histogram merging for accurate percentiles
- Progress reporting during distributed execution

**Future (v0.7.0+):**
- Real-time streaming metrics from agents
- Dynamic agent discovery
- Failure recovery (agent crashes)
- Load balancing across heterogeneous agents

## Risk Analysis

### Technical Risks

1. **Clock Synchronization**
   - **Risk**: Agents start at different times due to clock skew
   - **Mitigation**: Use controller's timestamp + 2-second buffer
   - **Impact**: Low - 2 seconds is enough for network latency

2. **Network Failures**
   - **Risk**: Agent unreachable during workload
   - **Mitigation**: Timeout + retry logic, fail fast on initial ping
   - **Impact**: Medium - operator must restart failed agents

3. **Path Rewriting Edge Cases**
   - **Risk**: Complex URIs break path joining logic
   - **Mitigation**: Comprehensive unit tests, support for `url` crate
   - **Impact**: Medium - test all backend types thoroughly

### Operational Risks

1. **Path Conflicts**
   - **Risk**: Operator manually specifies same paths
   - **Mitigation**: Clear documentation, warnings in output
   - **Impact**: Low - default behavior is safe (auto-isolation)

2. **Storage Cleanup**
   - **Risk**: Each agent creates data, cleanup may be incomplete
   - **Mitigation**: Prepare phase cleanup deletes per-agent paths
   - **Impact**: Low - existing cleanup logic works per-agent

## Success Metrics

### Functional Requirements ✅

- [ ] Controller can distribute YAML config to N agents
- [ ] Each agent executes in isolated path
- [ ] Coordinated start within 100ms across agents
- [ ] Results aggregated correctly
- [ ] All operation types supported (GET, PUT, LIST, STAT, DELETE)

### Performance Requirements ✅

- [ ] Overhead < 1% (distributed vs single-node for same total ops)
- [ ] Scales linearly to 10+ agents
- [ ] Network overhead < 1MB per agent (config + results)

### Quality Requirements ✅

- [ ] 90% test coverage for distributed code paths
- [ ] Documentation with examples
- [ ] Backward compatible with v0.5.x configs
- [ ] No breaking changes to single-node `sai3-bench` CLI

## Open Questions

1. **Histogram Aggregation**: How to merge HDR histograms from multiple agents?
   - Option A: Serialize histograms, merge on controller (accurate percentiles)
   - Option B: Report per-agent percentiles only (simpler, less accurate aggregate)
   - **Decision**: Start with Option B, add Option A in v0.7.0 if needed

2. **Agent Auto-Discovery**: Should agents register with a central service?
   - Option A: Static list of agents (current approach)
   - Option B: Service discovery (etcd, Consul, DNS SRV records)
   - **Decision**: Static list for v0.6.0, consider discovery in v0.7.0

3. **Failure Handling**: What if an agent crashes mid-workload?
   - Option A: Fail entire distributed run
   - Option B: Continue with remaining agents, report partial results
   - **Decision**: Fail fast for v0.6.0 (simpler semantics)

4. **Result Export**: How to export TSV from distributed run?
   - Option A: Separate TSV per agent
   - Option B: Merged TSV with agent_id column
   - Option C: Both options available
   - **Decision**: Option C - `--export-mode=separate|merged|both`

## Next Steps

1. **Create Feature Branch**: `git checkout -b feature/v0.6.0-distributed`
2. **Update Protobuf**: Add `RunWorkload` RPC and messages
3. **Implement Agent Handler**: Path rewriting + workload execution
4. **Implement Controller Command**: Config distribution + result aggregation
5. **Write Tests**: Unit + integration tests for distributed execution
6. **Documentation**: Update README, add distributed usage guide
7. **Release**: Merge to main, tag v0.6.0

## Timeline Estimate

- **Week 1**: Protocol + Agent implementation (15-20 hours)
- **Week 2**: Controller implementation + basic tests (15-20 hours)
- **Week 3**: Integration tests + documentation (10-15 hours)
- **Total**: ~40-55 hours of development

**Target Release**: End of October 2025

---

## Conclusion

The distributed workload feature is a natural evolution of sai3-bench's architecture. By leveraging the existing gRPC infrastructure and YAML config system, we can achieve powerful multi-host benchmarking with minimal complexity.

**Key Innovation**: Per-agent path isolation eliminates the coordination challenges of traditional distributed benchmarks, enabling truly independent parallel load generation while maintaining result aggregation.

This design maintains backward compatibility, reuses existing code, and sets the foundation for future enhancements like real-time metrics streaming and auto-scaling.
